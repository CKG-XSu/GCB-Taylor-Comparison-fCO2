{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ebcc079",
   "metadata": {},
   "source": [
    "install if your environment does not have cbsyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22b6e4f-eaa5-433a-a776-508b165ef5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install cbsyst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22309aa9-98ff-4357-957e-ca73ae9ff743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request as ureq\n",
    "from tqdm import tqdm\n",
    "import cbsyst as cb\n",
    "import xarray as xr\n",
    "import gcsfs\n",
    "import cmocean as cm\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1330607-d4c0-4dc2-9dbe-ef5eb31949f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the surface depth of the data\n",
    "surface_depth = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62205331-c50c-4dbd-bb11-b25681367c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://glodap.info/index.php/merged-and-adjusted-data-product-v2-2023/\n",
    "# Copy the link of \"Merged Master File (ZIPped CSV)\"\n",
    "\n",
    "# Download path\n",
    "glodap_url ='https://glodap.info/glodap_files/v2.2023/GLODAPv2.2023_Merged_Master_File.csv.zip'\n",
    "# GCS path to save the data\n",
    "save_path = 'YOUR_GCS_PATH/Taylor_data/databases'\n",
    "# Edit the file name if needed.\n",
    "gcs_path = f'{save_path}/raw/GLODAPv2.2023_Merged_Master_File.csv.zip'\n",
    "\n",
    "# If you want to change the outcome .nc save path or file name, search for \"zarr_gcs_path\" later in this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9177e3-8135-4953-8e67-eb67fe53f70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and save to GCS\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "\n",
    "# download and save to GCS\n",
    "if fs.exists(gcs_path):\n",
    "    print(f\"File already exists at {gcs_path}. Skipping download.\")\n",
    "else:\n",
    "    response = requests.get(glodap_url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with fs.open(gcs_path, 'wb') as gcs_file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:  # filter out keep-alive new chunks\n",
    "                    gcs_file.write(chunk)\n",
    "        print(f\"Succesfully downloaded and saved to {gcs_path}\")\n",
    "    else:\n",
    "        print(f\"Error! Could not download: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af92ec51-94bf-403a-85c1-5885c5a83373",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = gcsfs.GCSFileSystem()\n",
    "fs.ls('YOUR_GCS_PATH/Taylor_data/databases/raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296fe1f3-8ef4-4385-a74e-627f787bb827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from GCS\n",
    "# zip to csv\n",
    "zf = zipfile.ZipFile(fs.open(gcs_path))\n",
    "\n",
    "gd = pd.read_csv(zf.open('GLODAPv2.2023_Merged_Master_File.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6dff55-42be-4665-8b42-d90223590901",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove \"G2\" of all column names. Could be manually set if changes apply in the future\n",
    "gd.columns = gd.columns.str.replace('G2', '')\n",
    "\n",
    "gd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef80718-2d84-4bf0-b9d8-1611f4f970c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace missing values(-9999) with nan\n",
    "gd.replace(-9999, np.nan, inplace=True)\n",
    "\n",
    "# isolate good data only (flag = 2)\n",
    "# From https://essd.copernicus.org/articles/12/3653/2020/ \n",
    "# From the notes of table 1 and 2, we know that those column names end with \"f\" represent the WOCE flags and the flag = 2 means the data is acceptable.\n",
    "gd.loc[gd.phtsinsitutpf != 2, 'phtsinsitutp'] = np.nan\n",
    "gd.loc[gd.tco2f != 2, 'tco2'] = np.nan\n",
    "gd.loc[gd.talkf != 2, 'talk'] = np.nan\n",
    "gd.loc[gd.salinityf != 2, 'salinity'] = np.nan\n",
    "gd.loc[gd.phosphatef != 2, 'phosphate'] = np.nan\n",
    "gd.loc[gd.silicatef != 2, 'silicate'] = np.nan\n",
    "\n",
    "gd.dropna(subset=['phtsinsitutp', 'tco2', 'talk', 'temperature', 'salinity', 'pressure', 'silicate', 'phosphate'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7970388-1bfb-4664-8c05-861465951f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check what unique values are there in 'salinityf' column. Should only have \"2\". \n",
    "gd.salinityf.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42918295-21cc-449c-891e-70970e554c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put year/month/day/hour into sensible format\n",
    "gd['date'] = pd.to_datetime(gd[['year', 'month', 'day']])\n",
    "\n",
    "### Convert from -180 / 180 to 0/360 longitude\n",
    "def f(row):\n",
    "    if row['longitude'] < 0:\n",
    "        val = row['longitude'] + 360\n",
    "    else:\n",
    "        val = row['longitude']\n",
    "    return val\n",
    "\n",
    "### Apply function to dataframe \n",
    "gd['lon_0360'] = gd.apply(f, axis=1)\n",
    "\n",
    "### convert pressure to bar\n",
    "gd.pressure /= 10 \n",
    "\n",
    "### Calculate pCO2 using cbsyst\n",
    "calc_pco2 = cb.Csys(pHtot=gd.phtsinsitutp, DIC=gd.tco2, T_in=gd.temperature, S_in=gd.salinity, P_in=gd.pressure, PT=gd.phosphate, SiT=gd.silicate, BT=415.7)\n",
    "\n",
    "### put pCO2 into GLODAP dataframe - gd\n",
    "gd['pco2'] = calc_pco2.pCO2\n",
    "\n",
    "### Groupby profile and take index where depth is min\n",
    "gd = gd.loc[gd.groupby(['cruise','station','cast'])['depth'].idxmin()]\n",
    "\n",
    "### Set index to dates\n",
    "gd_sub = gd.loc[:, ['date', 'latitude', 'lon_0360', 'depth', 'pco2']].rename(columns={'latitude':'lat', 'lon_0360':'lon', 'pco2':'spco2', 'date':'time'}).set_index(['time'])\n",
    "\n",
    "### Only look in top 'surface_depth' meters, here we use 10m for now\n",
    "# Depth is defined earlier, search for \"surface_depth\" if you want to change it\n",
    "gd_sub = gd_sub.where(gd_sub['depth']<=surface_depth).dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccda315-c2dc-4920-8862-1305715f12ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gd_sub.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02893f86-2917-4727-b37c-36109a34f000",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a flat array of ungridded data and grids it to the grid of the given xr.DataArray\n",
    "\n",
    "# Date range to create monthly mask over\n",
    "# list of start and stop dates\n",
    "# Auto detect the date range. But could be manually set.\n",
    "min_date_str = gd_sub.index.min().strftime('%Y-%m')\n",
    "max_date_str = gd_sub.index.max().strftime('%Y-%m')\n",
    "date_range = [min_date_str,max_date_str]\n",
    "# date_range = ['1982-01', '2021-12']\n",
    "print(f'Date range is:{date_range[0]} to {date_range[1]}.')\n",
    "\n",
    "# Define spatial range\n",
    "lon = np.arange(0.5, 360, 1)\n",
    "lat = np.arange(-89.5, 90, 1)\n",
    "time = pd.date_range(start=f'{date_range[0]}-01T00:00:00.000000000',\n",
    "                     end=f'{date_range[1]}-01T00:00:00.000000000',\n",
    "                     freq='MS') + np.timedelta64(14, 'D')\n",
    "\n",
    "# Create DataArray\n",
    "xda = xr.DataArray(np.zeros((len(time), len(lat), len(lon))),\n",
    "                   dims=['time', 'lat', 'lon'],\n",
    "                   coords=[time, lat, lon])\n",
    "\n",
    "# Convert data into series\n",
    "ds_gd = pd.DataFrame({\n",
    "    'time': gd_sub.index,\n",
    "    'lon': gd_sub.lon.values,\n",
    "    'lat': gd_sub.lat.values,\n",
    "    'spco2': gd_sub.spco2.values\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d49ff80-213f-4bed-890a-13e75b78c427",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_gd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb5a723-ae79-4d88-a636-a6485de92714",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if coordinates are present and correctly formatted\n",
    "for key in ['time', 'lat', 'lon']:\n",
    "     assert key in xda.dims, '`{}` is not in the input DataArray'.format(key)\n",
    "assert all(np.diff(xda.time.values) > np.timedelta64(0, 'D')), 'time is not strictly increasing'\n",
    "assert all(np.diff(xda.lat.values) > 0), 'latitude is not strictly increasing'\n",
    "assert all(np.diff(xda.lon.values) > 0), 'longitude is not strictly increasing'\n",
    "assert np.issubdtype(ds_gd['time'].dtype, np.datetime64), 'time must be in datetime64 format'\n",
    "\n",
    "# Create bins for time, latitude, and longitude\n",
    "def make_bins(dim):\n",
    "    delta = np.diff(dim) / 2\n",
    "    bins = np.r_[dim[0] - delta[0], dim[1:] - delta, dim[-1] + delta[-1]]\n",
    "    return bins\n",
    "\n",
    "tbins = make_bins(xda.time.values)\n",
    "ybins = make_bins(xda.lat.values)\n",
    "xbins = make_bins(xda.lon.values)\n",
    "\n",
    "# Bin the data and get indices\n",
    "tidx = pd.cut(ds_gd['time'], tbins, labels=np.arange(len(xda.time)))\n",
    "yidx = pd.cut(ds_gd['lat'], ybins, labels=np.arange(len(xda.lat)))\n",
    "xidx = pd.cut(ds_gd['lon'], xbins, labels=np.arange(len(xda.lon)))\n",
    "\n",
    "print(\"Number of NaNs in bin indices:\")\n",
    "print(f\"tidx: {tidx.isna().sum()}, yidx: {yidx.isna().sum()}, xidx: {xidx.isna().sum()}\")\n",
    "\n",
    "# Group by bins and calculate mean and standard deviation\n",
    "df = (pd.DataFrame({'spco2': ds_gd['spco2'], 'tidx': tidx, 'yidx': yidx, 'xidx': xidx})\n",
    "      .dropna()  # Drop rows where binning failed\n",
    "      .groupby(['tidx', 'yidx', 'xidx'])\n",
    "      .agg(['mean', 'std'])\n",
    "      .reset_index()\n",
    "      .rename(columns={'tidx': 'it', 'yidx': 'iy', 'xidx': 'ix'}))\n",
    "'''\n",
    "df = (pd.Series(ds_gd.spco2)\n",
    "          .groupby([tidx, yidx, xidx])\n",
    "          .agg(['mean', 'std'])\n",
    "          .reset_index()\n",
    "          .rename(columns={'level_0': 'it',\n",
    "                           'level_1': 'iy',\n",
    "                           'level_2': 'ix'}))\n",
    "'''\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c18c1-b730-43d0-a23d-0284d4698479",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37cc275-bd0f-48cc-b52d-35a6bc4bb1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert indices to integers\n",
    "it, iy, ix = df[['it', 'iy', 'ix']].values.T.astype(int)\n",
    "\n",
    "# Create DataArray to store the results\n",
    "xda_mean = xda.copy() * np.nan\n",
    "xda_std = xda.copy() * np.nan\n",
    "xda_mean.name = 'spco2_mean'\n",
    "xda_std.name = 'spco2_std'\n",
    "\n",
    "# Place the binned mean and standard deviation into the DataArray\n",
    "xda_mean.values[it, iy, ix] = df[('spco2', 'mean')]\n",
    "xda_std.values[it, iy, ix] = df[('spco2', 'std')]\n",
    "\n",
    "# Merge into a single Dataset\n",
    "out = xr.merge([xda_mean, xda_std])\n",
    "\n",
    "# Rename variables\n",
    "glodap_out = out.rename({'spco2_mean': 'spco2_mean', 'spco2_std': 'spco2_std'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2339bc8-1427-45a0-a6ba-df430a8565be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to GCS as zarr\n",
    "\n",
    "# Detect the year of GLODAP file name\n",
    "filename = gcs_path.rsplit('/', 1)[-1]\n",
    "match = re.search(r'\\d{4}', filename)\n",
    "if match:\n",
    "    updated_year = match.group()\n",
    "else:\n",
    "    print(\"Fail to detect the year info from the GLODAP file name!\")\n",
    "\n",
    "# Set the save path and name of processed netCDF file\n",
    "zarr_gcs_path = f'{save_path}/processed/GLODAPv2_spco2_{updated_year}.zarr'\n",
    "\n",
    "glodap_out.to_zarr( zarr_gcs_path, mode='w')\n",
    "\n",
    "# Check if saved sucessfully\n",
    "if fs.exists(zarr_gcs_path):\n",
    "    print(f\"Successfully saved to {zarr_gcs_path}\")\n",
    "else:\n",
    "    print(f\"Failed to save to {zarr_gcs_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9189f9d4-2b43-4054-9cba-cb18f9628ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls(save_path + '/processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f1cb6b-38c3-4722-8270-d23c252a195f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "glodap_out.spco2_mean.sel(time = '2021-05').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcf691a-a50c-4b75-adc0-dcee7ef548f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to delete file from the GCS path, run the following code\n",
    "'''\n",
    "delete_path = 'YOUR_GCS_PATH/Taylor_data/GLODAPv2.2023_Merged_Master_File.csv'\n",
    "\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "\n",
    "if fs.exists(delete_path):\n",
    "    fs.rm(delete_path, recursive=True)\n",
    "    print(f\"The file {delete_path} has been deleted\")\n",
    "else:\n",
    "    print(f\"The file {delete_path} does not exist\")\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
